\begin{abstract}

The \textit{LLM-as-a-judge} paradigm offers a potential solution to scalability issues in human evaluation of large language models (LLMs), but there are still many open questions about its strengths, weaknesses, and potential biases. This study investigates \njudgesword models, ranging in size and family, as `\textit{\judgemodels}' evaluating answers from \nexamtakersword base and instruction-tuned `\textit{\evaluatormodels}'. We find that only the best (and largest) models show reasonable alignment with humans, though they still differ with up to 5 points from human-assigned scores. Our research highlights the need for alignment metrics beyond percent agreement, as judges with high agreement can still assign vastly different scores. We also find that smaller models and the lexical metric \judge{contains} can provide a reasonable signal in ranking the \evaluatormodels. Further error analysis reveals vulnerabilities in \judgemodels, such as sensitivity to prompt complexity and a bias toward leniency. Our findings show that even the best \judgemodels differ from humans in this fairly sterile setup, indicating that caution is warranted when applying \judgemodels in more complex scenarios.

% Offering a promising solution to the scalability challenges associated with human evaluation, the \textit{LLM-as-a-judge} paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs).
% However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold.
% In this paper, we present a comprehensive study of the performance of various LLMs acting as judges
% % \footnote{Source code is available at \url{https://github.com/UMass-Meta-LLM-Eval/llm_eval}}
% % We leverage TriviaQA as a benchmark for assessing objective knowledge reasoning of LLMs and evaluate them alongside human annotations which we found to have a high inter-annotator agreement. 
% Investigating \njudgesword \textit{\judgemodels} of different model sizes and families, judging answers of \nexamtakersword different `\textit{\evaluatormodels}' -- both base and instruction-tuned -- we find that only the best (and largest) models achieve reasonable alignment with humans.
% However, they are still quite far behind inter-human agreement and their assigned scores may still differ with up to 5 points from human-assigned scores.
% In terms of their ranking of the \nexamtakersword \evaluatormodels, instead, also smaller models and even the lexical metric \judge{contains} may provide a reasonable signal.
% Through error analysis and other studies, we identify vulnerabilities in \judgemodels, such as their sensitivity to prompt complexity and length, and a tendency toward leniency.
% The fact that even the best judges differ from humans in this comparatively simple setup suggest that caution may be wise when using judges in more complex setups.
% Lastly, our research rediscovers the importance of using alignment metrics beyond simple percent alignment, showing that judges with high percent agreement can still assign vastly different scores.

% We find that \judge{Llama-3\;70B}, \judge{Llama-3.1\;70B} and \judge{GPT-4\;Turbo} have an excellent alignment with humans, but in terms of \textit{ranking} exam taker models, they are not better than some smaller models and even the lexical matching method \judge{Contains}, which have up to 24 points lower human alignment. 
% Furthermore, the basic lexical \judge{Contains} match and the fine-tuned \judge{JudgeLM-7B} maintain the \textit{ranking} of \evaluatormodels better than larger and highly aligned models like \judge{\gpt} and \judge{Llama-3 70B}, despite having kappa scores up to 34 points lower than human alignment. 
% We find that both Llama-3-70B and GPT-4 have excellent alignment with humans, but in terms of \textit{ranking} \evaluatormodels, they are outperformed by both JudgeLM-7B and the lexical matching method \judge{contains}.
% Finally, our error analysis highlights vulnerabilities inherent in judgemodels.

% Through error analysis and various other studies, including the effects of instruction length and leniency bias, we hope to provide useful lessons for using LLMs as judges in the future.

% We discover that both Llama-3 and GPT-4 have very high alignment with humans, does this alignment 
% Our research uncovers substantial misalignment between \judgemodels and human annotations.  
% We demonstrate that although models such as Llama 3 and GPT-4 exhibit high alignment, this alignment does not correlate with comparable evaluation scores. 
% Furthermore, we observe that basic lexical matching techniques more precisely replicate the rankings of exam-taker models assigned by humans compared to all \judgemodels.
% Finally, our error analysis highlights vulnerabilities inherent in \judgemodels, particularly in instruction-tuned models.Further examination of their prompt sensitivities and biases reveals that they are often lenient and susceptible to manipulation.
\end{abstract}
